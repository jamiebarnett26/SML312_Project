# -*- coding: utf-8 -*-
"""Data Analyis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KE6fzZbovBM6Zyb7nMHnlAZC13I71I8e
"""

import pandas as pd

people = pd.read_csv("clean_people.csv", engine='python', on_bad_lines='skip')
calls = pd.read_csv("clean_calls.csv")
appointments = pd.read_csv("clean_appointments.csv")
events = pd.read_csv("clean_events.csv", engine='python', on_bad_lines='skip')
emEvents = pd.read_csv("clean_emEvents.csv")
users = pd.read_csv("clean_users.csv")
texts = pd.read_csv("cleaned_textMessages.csv")

import pandas as pd
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
people['stage'].value_counts().plot(kind='bar', color='skyblue')
plt.title('Lead Stage Distribution')
plt.xlabel('Stage')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 6))
people['claimed'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('Claimed vs Unclaimed Leads')
plt.ylabel('')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
people['created'] = pd.to_datetime(people['created'])
people['created'].dt.date.value_counts().sort_index().plot(kind='line')
plt.title('Leads Created Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Leads')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
calls['outcome'].value_counts(dropna=False).plot(kind='bar', color='salmon')
plt.title('Call Outcomes')
plt.xlabel('Outcome')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
calls['duration'] = pd.to_numeric(calls['duration'], errors='coerce')
calls['duration'].dropna().plot(kind='hist', bins=30, color='lightgreen')
plt.title('Call Duration Distribution')
plt.xlabel('Duration (seconds)')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
appointments['created'] = pd.to_datetime(appointments['created'])
appointments['created'].dt.date.value_counts().sort_index().plot(kind='line', color='purple')
plt.title('Appointments Created Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Appointments')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
appointments['outcome'].value_counts(dropna=False).plot(kind='bar', color='orange')
plt.title('Appointment Outcomes')
plt.xlabel('Outcome')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
events['type'].value_counts().plot(kind='bar', color='gold')
plt.title('Event Types')
plt.xlabel('Event Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
events['source'].value_counts().plot(kind='bar', color='coral')
plt.title('Event Sources')
plt.xlabel('Source')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
emEvents['type'].value_counts().plot(kind='bar', color='teal')
plt.title('Email Event Types')
plt.xlabel('Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
notes['personId'].value_counts().head(20).plot(kind='bar', color='lightblue')
plt.title('Top 20 People by Number of Notes')
plt.xlabel('Person ID')
plt.ylabel('Number of Notes')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
users['role'].value_counts().plot(kind='bar', color='lightcoral')
plt.title('User Roles Distribution')
plt.xlabel('Role')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 6))
users['status'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)
plt.title('User Status')
plt.ylabel('')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
notes['type'].value_counts(dropna=False).plot(kind='bar', color='plum')
plt.title('Notes by Type')
plt.xlabel('Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
notes['created'] = pd.to_datetime(notes['created'])
notes['created'].dt.date.value_counts().sort_index().plot(kind='line', color='navy')
plt.title('Notes Created Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Notes')
plt.tight_layout()
plt.show()

top_clients = client_metrics_df.sort_values('total_calls', ascending=False).head(20)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

sns.barplot(x='personId', y='total_calls', data=top_clients, ax=axes[0, 0])
axes[0, 0].set_title('Top 20 Clients by Total Calls')

sns.barplot(x='personId', y='response_rate', data=top_clients, ax=axes[0, 1])
axes[0, 1].set_title('Text Response Rate')

sns.barplot(x='personId', y='email_opens', data=top_clients, ax=axes[0, 2])
axes[0, 2].set_title('Email Opens')

sns.barplot(x='personId', y='days_since_last_activity', data=top_clients, ax=axes[1, 1])
axes[1, 1].set_title('Days Since Last Activity')

sns.barplot(x='personId', y='calls_made', data=merged_df[merged_df['personId'].isin(top_clients['personId'])], ax=axes[1, 2])
axes[1, 2].set_title('Calls Made')

for ax in axes.flat:
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5, annot_kws={"size": 10}, cbar_kws={"shrink": 0.75})
plt.title("Feature Correlation Matrix", fontsize=16)
plt.tight_layout()
plt.show()

if 'people' in globals():
    stage_counts = people['stage'].value_counts(dropna=False)
    print("Unique stages and their counts:\n")
    print(stage_counts)
else:
    print("The 'people' DataFrame is not loaded. Please load it first.")

plt.figure(figsize=(10, 6))
sns.scatterplot(x='calls_made', y='appointments_set', data=merged_df, hue='stage', palette='viridis')
plt.title("Calls Made vs Appointments Set")
plt.xlabel('Calls Made')
plt.ylabel('Appointments Set')
plt.legend(title='Stage', loc='upper right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='stage', y='calls_made', data=merged_df)
plt.title("Distribution of Calls Made Across Stages")
plt.xlabel('Stage')
plt.ylabel('Calls Made')
plt.tight_layout()
plt.show()

import ast
import pandas as pd

incoming_calls = calls[calls['isIncoming'] == True]
outgoing_calls = calls[calls['isIncoming'] == False]
incoming_texts = texts[texts['isIncoming'] == True]
outgoing_texts = texts[texts['isIncoming'] == False]

incoming_calls_count = incoming_calls.groupby('personId')['id'].count().reset_index(name='in_calls_count')
outgoing_calls_count = outgoing_calls.groupby('personId')['id'].count().reset_index(name='out_calls_count')
incoming_texts_count = incoming_texts.groupby('personId')['id'].count().reset_index(name='in_texts_count')
outgoing_texts_count = outgoing_texts.groupby('personId')['id'].count().reset_index(name='out_texts_count')

def extract_person_ids(invitees):
    try:
        invitees_list = ast.literal_eval(invitees)
        return [i.get('personId') for i in invitees_list if i.get('personId') is not None]
    except (ValueError, SyntaxError, TypeError):
        return []

appointments['personIds'] = appointments['invitees'].apply(extract_person_ids)
appointments_exploded = appointments.explode('personIds')
appointments_exploded = appointments_exploded[appointments_exploded['personIds'].notnull()]
appointments_count = appointments_exploded.groupby('personIds').size().reset_index(name='appointment_count')

emails_per_person = emEvents.groupby('personId').size().reset_index(name='emails_count')

people = people.rename(columns={'id': 'personId'})
df_people = people.copy()

df_people = df_people.merge(incoming_calls_count, how='left', on='personId')
df_people = df_people.merge(outgoing_calls_count, how='left', on='personId')
df_people = df_people.merge(incoming_texts_count, how='left', on='personId')
df_people = df_people.merge(outgoing_texts_count, how='left', on='personId')
df_people = df_people.merge(emails_per_person, how='left', on='personId')

if not appointments_count.empty:
    df_people = df_people.merge(appointments_count, how='left', left_on='personId', right_on='personIds')
    df_people.drop(columns=['personIds'], inplace=True, errors='ignore')

fill_columns = ['in_calls_count', 'out_calls_count', 'in_texts_count', 'out_texts_count', 'emails_count', 'appointment_count']
for col in fill_columns:
    if col not in df_people.columns:
        df_people[col] = 0
    else:
        df_people[col] = df_people[col].fillna(0)

df_people['appointment_count'] = df_people['appointment_count'].apply(lambda x: 1 if x > 0 else 0)

def compute_agent_client_frequency(df_source, name):
    df_source['date'] = pd.to_datetime(df_source['created'], errors='coerce')
    df_source = df_source.dropna(subset=['personId', 'date'])
    agg = df_source.groupby('personId')['date'].agg(['min', 'max', 'count']).reset_index()
    agg['duration_days'] = (agg['max'] - agg['min']).dt.days.replace(0, 1)

    agg['duration_weeks'] = agg['duration_days'] / 7
    agg[f'{name}_per_week'] = agg['count'] / agg['duration_weeks']
    return agg[['personId', f'{name}_per_week']]

out_calls_time = compute_agent_client_frequency(outgoing_calls, 'out_calls')
out_texts_time = compute_agent_client_frequency(outgoing_texts, 'out_texts')
emails_time = compute_agent_client_frequency(emEvents, 'emails')
in_calls_time = compute_agent_client_frequency(incoming_calls, 'in_calls')
in_texts_time = compute_agent_client_frequency(incoming_texts, 'in_texts')

df_people_time = df_people.copy()
for df_time in [out_calls_time, out_texts_time, emails_time, in_calls_time, in_texts_time]:
    df_people_time = df_people_time.merge(df_time, on='personId', how='left')

df_people_time['created'] = pd.to_datetime(df_people_time['created'], errors='coerce').dt.tz_localize(None)
df_people_time['tenure_days'] = (pd.to_datetime('today').tz_localize(None) - df_people_time['created']).dt.days
df_people_time['tenure_days'] = df_people_time['tenure_days'].fillna(0).replace(0, 1)
df_people_time['stage_success'] = df_people_time['stage'].str.lower().isin(['closed', 'spoke with customer', 'appointment set', 'met with customer', 'showing homes', 'submitting offers']).astype(int)

df_people_time.fillna(0, inplace=True)

print(df_people_time.head())

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

df_people_time['closed'] = df_people_time['stage'].apply(lambda x: 1 if x == 'Closed' else 0)
df_clean = df_people_time[['out_calls_count', 'out_texts_count', 'emails_count','in_calls_count', 'in_texts_count', 'closed']].copy()
corr = df_clean.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, vmin=-1, vmax=1)
plt.title('Correlation: Behavior Count & Closed Deals')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

df_people_time['total_interactions'] = (
    df_people_time['out_calls_count'] +
    df_people_time['out_texts_count'] +
    df_people_time['emails_count']
)

df_people_time['weekly_frequency'] = (
    df_people_time['out_calls_per_week'] +
    df_people_time['out_texts_per_week'] +
    df_people_time['emails_per_week']
)

fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)
sns.set(style='whitegrid')

sns.scatterplot(
    x='tenure_days',
    y='total_interactions',
    data=df_people_time,
    ax=axs[0],
    alpha=0.6,
    color='orange'
)

axs[0].set_title('Total Interactions vs. Tenure (Biased)', fontsize=13)
axs[0].set_xlabel('Tenure (days)')
axs[0].set_ylabel('Total Interactions')
axs[0].legend()

sns.scatterplot(
    x='tenure_days',
    y='weekly_frequency',
    data=df_people_time,
    ax=axs[1],
    alpha=0.6,
    color='steelblue'
)
axs[1].set_title('Weekly Frequency vs. Tenure (Time-Normalized)', fontsize=13)
axs[1].set_xlabel('Tenure (days)')
axs[1].set_ylabel('Weekly Frequency')
axs[1].legend()

plt.suptitle('Figure 2: Time Bias in Raw Counts vs. Time-Normalized Features', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

df_people_time['closed'] = df_people_time['stage'].apply(lambda x: 1 if x == 'Closed' else 0)

df_filtered = df_people_time[df_people_time['in_texts_per_week'] < df_people_time['in_texts_per_week'].quantile(0.99)]

plt.figure(figsize=(10, 6))
sns.violinplot(x='closed', y='in_texts_per_week', data=df_filtered, inner='box', palette='Set2')

plt.title('Distribution of Incoming Texts per Day by Deal Closure')
plt.xlabel('Closed Deal (1 = Yes, 0 = No)')
plt.ylabel('Incoming Texts per Day')
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import shap
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (
    classification_report, roc_auc_score,
    precision_recall_curve, make_scorer, recall_score
)
from skopt import BayesSearchCV

X = df_people_time[['out_calls_per_week', 'out_texts_per_week', 'emails_per_week']].fillna(0)
y = df_people_time['stage_success'].fillna(0)

X_unscaled = X.copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train_scaled, X_test_scaled, y_train, y_test, X_train_unscaled, X_test_unscaled = train_test_split(
    X_scaled, y, X_unscaled, stratify=y, test_size=0.2, random_state=42
)

def evaluate_model(model, model_name, X_train_raw, X_test_raw):
    proba = model.predict_proba(X_test_scaled)[:, 1]
    precision, recall, thresholds = precision_recall_curve(y_test, proba)
    f1_scores = 2 * precision * recall / (precision + recall + 1e-10)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    preds_opt = (proba >= optimal_threshold).astype(int)

    print(f"\n--- {model_name} ---")
    print(f"ROC AUC: {roc_auc_score(y_test, proba):.4f}")
    print(f"Optimal Threshold: {optimal_threshold:.3f}")
    print("Classification Report:")
    print(classification_report(y_test, preds_opt))

    plt.figure(figsize=(6, 4))
    plt.plot(recall, precision, marker='.', label=model_name)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve: {model_name}')
    plt.legend()
    plt.grid(True)
    plt.show()

    try:
        if isinstance(model, (RandomForestClassifier, XGBClassifier)):
            explainer = shap.Explainer(model, X_train_raw)
            shap_values = explainer(X_test_raw)
        else:
            explainer = shap.Explainer(model, X_train_scaled)
            shap_values = explainer(X_test_scaled)

        feature_names = ['out_texts_per_week', 'out_calls_per_week', 'emails_per_week']

        class_1_idx = y_test.values == 1
        class_0_idx = y_test.values == 0

        mean_shap_success = np.abs(shap_values[class_1_idx].values).mean(axis=0)
        mean_shap_fail = np.abs(shap_values[class_0_idx].values).mean(axis=0)

        plt.figure(figsize=(6, 4))
        plt.barh(feature_names, mean_shap_success)
        plt.title(f'{model_name} - Feature Importance (Success)')
        plt.xlabel('Mean |SHAP value|')
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(6, 4))
        plt.barh(feature_names, mean_shap_fail, color='orange')
        plt.title(f'{model_name} - Feature Importance (Not Success)')
        plt.xlabel('Mean |SHAP value|')
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"[WARNING] SHAP explanation skipped for {model_name}: {e}")


logreg = LogisticRegression(class_weight='balanced', random_state=42, solver='liblinear')
search_space = {
    'C': (1e-4, 1e2, 'log-uniform'),
    'penalty': ['l1', 'l2']
}
bayes_logreg = BayesSearchCV(
    logreg, search_spaces=search_space, n_iter=30,
    scoring=make_scorer(recall_score, pos_label=1),
    cv=3, n_jobs=-1, random_state=42
)
bayes_logreg.fit(X_train_scaled, y_train)
evaluate_model(bayes_logreg.best_estimator_, "Logistic Regression", X_train_scaled, X_test_scaled)

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_scaled, y_train)
evaluate_model(xgb, "XGBoost", X_train_unscaled, X_test_unscaled)